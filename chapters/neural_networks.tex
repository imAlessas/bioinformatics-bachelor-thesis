%!TEX root = ../main.tex

\chapter{Reti neurali}\label{chp:neural-networks}
% 
Il primo modello di intelligenza artificiale risale al 1943, dove E. McCulloch e W. Pitts cercarono di modellare un neurone come una semplice funzione predefinita. Nel modello, il neurone, generava un valore in output nel caso in cui le variabili booleane di input, una volta elaborate, superavano una soglia prestabilita\,\cite[``A logical calculus of the ideas immanent in nervous activity'']{mcculloch1943logical}. Poco dopo, nel 1950, Alan Turing, pubblicò un articolo che definiva una metodologia per testare l'intelligenza di un modello\,\cite[``Computing machinery and intelligence'']{turing2009computing}. Questo test — noto anche come \textsl{The imitation game} — consisteva nel valutare se una macchina potesse imitare l'intelligenza umana tabilendo così un obiettivo per il campo dell'intelligenza artificiale, termine che venne conianto per la prima volta nella conferenza di Dartmouth nel 1956.

Dopo due anni, nel 1958, lo psicologo F. Resenblatt introdusse il \textsl{percettrone} che, a differenza del modello del '43, processava input non booleani e disponeva di pesi per bilanciare l'output\,\cite[``The perceptron: a probabilistic model for information storage and organization in the brain.'']{rosenblatt1958perceptron}. Anche se il percettrone sarà alla base delle reti neurali artificiali moderne, nei dieci anni successivi alla pubblicazione dell'articolo, le aspettative iniziali non vennero soddisfatte. Nel 1968 venne pubblicato un libro il quale analizzava le prestazioni del percettrone e constatava le forti limitazioni del modello, come l'impossibilità di risolvere problemi non linearmente separabili\,\cite[``Perceptrons'']{minsky2017perceptrons}. In seguito ad un secondo articolo del 1973, dove si evidenziavano gli scarsi risultati ottenuti in paragone con le grandi aspettative, iniziò il \textsl{Primo Inverno dell'Intelligenza Artificiale} dove fino alla metà degli anni Ottanta molte organizzazioni governative smisero di finanziare la ricerca sull'\ac{AI}. L'Inverno della \ac{AI} terminò nel 1985 con l'introduzione del \textit{Gradient Descent Optimization}, algoritmo che permetteva di aggiornare i pesi in modo tale da minimizzare l'errore in una rete. Un anno dopo venne introdotto l'algoritmo della \textit{back-propagation}, fondamentale per lo sviluppo di reti neurali, costituite da più livelli di neuroni, ciascuno dei quali è collegato al livello successivo\,\cite[``Learning representations by back-propagating errors'']{rumelhart1986learning}.

Nonostante il grande sviluppo nella parte algoritmistica, l'hardware non era computazionalmente prestante da supportare le richieste di calcolo delle reti neurali artificiali. Questa carenza nella potenza di calcolo portò al \textsl{Secondo Inverno dell'Intelligenza Artificiale}, periodo in cui l'interesse scientifico si spostò su modelli che richiedevano meno potenza di calcolo, come le \textit{Support Vector Machines} introdotte nel 1963. Il Secondo Inverno della \ac{AI} terminò a metà degli anni Novanta quando il progresso dell'hardware riuscì a soddisfare i requisiti computazionali dei modelli basati su reti neurali. Il costante sviluppo culminò nell'ultimo ventennio quando venne introdotta la GPU, che, insieme all'aumento dei dati disponibili, accelerò notevolmente i progressi nel campo dell'\ac{AI}\,\cite{flasinski2016introduction, muthukrishnan2020brief}.

\section{Principi di base ed evoluzione}
% 





\section{Reti neurali convoluzionali}
% 




\todo{quando fai CNN dici che ML fa cagare per questi}
\todo{Deep learning:      https://www.nature.com/articles/nature14539}

\begin{comment}
Il primo modello di intelligenza artificiale tento di creare il funzionamento di un neurone (ce la foto se vuoi metterla). I primi modelli piu semplici furono delle semplicit input-output function. IN seguito le funzioni divennero piu complicate, piu livelli furono aggiunti e feedback bidiriezionale fino ad arrivare ai modelli odierni di DL.

\@ La prima pubblicazione sulla \ac{AI}, fu del 43 (\cite{mcculloch1943logical}) che descrive un modello di computer che impara attraverso un processo paragonabile a quello dei neuroni: vengono quindi introdotti i neuroni MCP che prendono il nome dai due creatori (McCulloch and Pitts). Questo neuropne prendeva in input delle varibili booleane, processarle attraverso una funzione scelta a priori e poit, se il risultato superava una certa threshold, l'output avra un valore. Quyando la threshold è superata si diuce che il neurone si attiva. Aveva molte limitazioni, generava un output binario e richiedava un numero fisso di pesi.

Uno dei primi articoli che introducono una metodologia per testare l'intelligenza di un modello fu il testo di turing nel 1950. Questo test, chiamato Test di Turing, si domandava se una macchina fosse in grado di imitare l'intelligenza umana (\cite[Computing machinery and intelligence]{turing2009computing}). Il test prende in causa un interrogatore umano che fa la stessa domanda a due ascoltatori, una persona ed una macchina. Se l'interrogatopra non riesce a distingure l'uomo dalla macchina allora la macchina ha superato il test di Turing. Questo test è stato da sempre l'obiettivo dellàintelligenza artificiale anche se ad oggi alcuni dubbi stanno sorgendo. La conferenza del 1956 a Dartmouth (organizzata da Marvin Minsky, John McCarty, Claude Shannon e Nathan Rochester) è considerata il momento in cui l'\ac{AI} è stata globalmente riconosciuta. 

Un metodo piu sofisticato venne introdotto nerl 1958 da Resenblatt, chiamato percettrone il quale precessava input non booleani e pesi per bilanciare. Una funzione non lineare processa la somma dei prodotto degli input e dei pesi che rende il modell piu flessibile: questo sara la base per le reti neurali.

Il percettorne di Resenblatt aveva fatto molto clamore ma le aspettative dell'opinione pubblica non furoino incontrate. Le aspettative del pubblico non furono assolutamente incontrata, tanto che nel 68 un paper di Minsky e Papert dimostro le forte limitazioni del percettrone, dove di fatto veniva descritto che il percettrone non sarebbe stato in grado di emulare una operazione di or esclusivo con due input (non erano linearmente separabili). Nel 73 una seconda pubblicazione di Lighthill sottolinea le forti aspettativde del pubblico e gli scarsi risultati ottenuti dal progresso. Dal 74 all'80 ci fu il primo invero dell'intelligenza artificale dove essenzialmente nulla accadde e tutti persereo l'interesse al riogurado (controllo bene le fonto che qua me la sono un po inventata ecco letsgoski). 

Nel 1985 l'inverdo dell'ai fini quando venne introdotto il Gradent Descent Optimization per minimizzare l'errore in una rete (Rumelhart, Hinton and Williams). Inoltre nell 86 Rumelhart e colleghi espansero il lavoro introducento il conetto di back-propagation nelle reti neurali multilivello (molti livelli di neuroni attaxccati tra loro). QUersto algoritmo rivoluziono le capacita di imparare delle reti. Il secondo inverno dell \ac{AI} inizio negli anni 90, dove ci si rese conto che queste reti neurali non erano scalabili: cera troppa poca potenza di calcolo.

Questo inverno venne causato dalle aspettative sulle capacita delle reti neurali che pero non abdavanbo di paripasso con lo sviluppo della potenza computazionale. Per questo motivo i ricercatori si spostaro su altri alogirtimi, che richiedevano meno potenza di calcolo come le support evctor machines (1963, Vapnik and Chervonenkis). Quando quiesti algoritmi furono implementati in kernel non lineari nel 92 (kernl trick) si fu in grado di rsolvere iperpiano non lineari senza la necessita di requisiti computazionali elevati. 

Quando alla meta degli anni 90 la potenza computazionale crebbe, l'attenzione per la \ac{AI} auimeto di conseguenza. Nel 97, IBM sviluppo un super computer, chiamato Deep Blue, che sconfisse Kasparov a scacchi. Quersto evento rese le reti neurali risolrgere, con l'introduzioned elle reti nmeurali convoluzionali.

Nel 98 venne pubblicata LeNet-5, che è una rete convoluzionale a sette livelli. Viene usata la convoluzione per fare subsampling per poi passae ai livelli fully connected per predirre l'output. Anche in questo caso il modello era difficile da far scalare a causa di harder e data contstraints. Questo problema continuo fino all'ultimo decennio, dove due grandi avanzamente furono cruciali: lo storage dei dati e la GPU (graphical processing unit). I dati furono più accessibili e con meno costi. COn un alto livello di dati disponibili, la performance degli algoritmi di madhine learnign amutneto.

Questo introdusse il deeplearninng, nel 2006 sbloccando una performance eccellente nella speech recognition che prima era molto difficile. La nascita delle GPU introdusse una potenza di calcolo tale per cui si potevano introdurre livelli molto più complessi 
\end{comment}

\begin{comment}
Nel 1950 Alan turing pubbliuca ``Computing machinery and intewlligence'' dove viene introdotto il concetto di macchina intelligente. Viene introdotto anche il test di turing che è un test che determina se una mchhina e piu o meno capace di imitare l'uomo. Nel 1956, la onferenza di Darmont (c'è Shanno, Minsky che copnia il temrine intelligenza aertrifixcale). Gli obbbiettivi era quelli di machine translation. Rosenblat negli anni 60 si inveta il percettrone, rete neruale semplicissima. Agli anni 60 seguono due risultati allo stop dell IA, vhiamto inverno della intelligenza artificiazle. Minsky pubblica un libro, chiamato perceptrons dove viene dimostrato che il percettoprne èin grado di risolvere solo probleimi semplic, libnearmente seaprabili. Esce un report, Alpac, dove viene osservato che nemmeno la task di traduzione funziona.

Dagli anni 80, viene inventata la backpropagation dove le reti neurali diventano efficaci. Qui nascono anche i sistemi esperti, che sono dei sistemi che, per esempio, si basano su delle regole logiche: qualcosa dis uper dettagliato ed un esperto inietta dentro la sua conoscienza. Nel 1996/1997 Gary kasparov sfidas DeepBLUE, progettato da IBM.\@ La prima sfida, Kasparov vince il mach overall ma perde due partite. Nella seconda edizione del 97, deepBLUE vince tutto. Per la prima volta, un esperto viene sconmfitto da una macchina. Negli anni 2000 arriva il machine learning: apprendimento automatico. Una macchina che apprende dai dati.  Reti neurali sono un sottoinsieme del ML, il deep learning sono un sottinsieme di rete neruale (LLM sono un soittinseeme di deep NN). 
\end{comment}