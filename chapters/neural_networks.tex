%!TEX root = ../main.tex

\chapter{Reti neurali}\label{chp:neural-networks}
% 


\section{Principi di base ed evoluzione}
% 


\section{Reti neurali convoluzionali}
% 

\section*{Video Enkk}

Nel 1950 Alan turing pubbliuca ``Computing machinery and intewlligence'' dove viene introdotto il concetto di macchina intelligente. Viene introdotto anche il test di turing che è un test che determina se una mchhina e piu o meno capace di imitare l'uomo. Nel 1956, la onferenza di Darmont (c'è Shanno, Minsky che copnia il temrine intelligenza aertrifixcale). Gli obbbiettivi era quelli di machine translation. Rosenblat negli anni 60 si inveta il percettrone, rete neruale semplicissima. Agli anni 60 seguono due risultati allo stop dell IA, vhiamto inverno della intelligenza artificiazle. Minsky pubblica un libro, chiamato perceptrons dove viene dimostrato che il percettoprne èin grado di risolvere solo probleimi semplic, libnearmente seaprabili. Esce un report, Alpac, dove viene osservato che nemmeno la task di traduzione funziona.

Dagli anni 80, viene inventata la backpropagation dove le reti neurali diventano efficaci. Qui nascono anche i sistemi esperti, che sono dei sistemi che, per esempio, si basano su delle regole logiche: qualcosa dis uper dettagliato ed un esperto inietta dentro la sua conoscienza. Nel 1996/1997 Gary kasparov sfidas DeepBLUE, progettato da IBM.\@ La prima sfida, Kasparov vince il mach overall ma perde due partite. Nella seconda edizione del 97, deepBLUE vince tutto. Per la prima volta, un esperto viene sconmfitto da una macchina. Negli anni 2000 arriva il machine learning: apprendimento automatico. Una macchina che apprende dai dati.  Reti neurali sono un sottoinsieme del ML, il deep learning sono un sottinsieme di rete neruale (LLM sono un soittinseeme di deep NN). 

\section*{Libro}

Il primo modello di intelligenza artificiale tento di creare il funzionamento di un neurone (ce la foto se vuoi metterla). I primi modelli piu semplici furono delle semplicit input-output function. IN seguito le funzioni divennero piu complicate, piu livelli furono aggiunti e feedback bidiriezionale fino ad arrivare ai modelli odierni di DL.\@ La prima pubblicazione sulla AI, fu del 43 (\cite{mcculloch1943logical}) che descrive un modello di computer che impara attraverso un processo paragonabile a quello dei neuroni: vengono quindi introdotti i neuroni MCP che prendono il nome dai due creatori (McCulloch and Pitts). Questo neuropne prendeva in input delle varibili booleane, processarle attraverso una funzione scelta a priori e poit, se il risultato superava una certa threshold, l'output avra un valore. Quyando la threshold è superata si diuce che il neurone si attiva. Aveva molte limitazioni, generava un output binario e richiedava un numero fisso di pesi.

Un metodo piu sofisticato venne introdotto nerl 1958 da Resenblatt, chiamato percettrone il quale precessava input non booleani e pesi per bilanciare. Una funzione non lineare processa la somma dei prodotto degli input e dei pesi che rende il modell piu flessibile: questo sara la base per le reti neurali.

Uno dei primi articoli che introducono una metodologia per testare l'intelligenza di un modello fu il testo di turing nel 1950. Questo test, chiamato Test di Turing, si domandava se una macchina fosse in grado di imitare l'intelligenza umana (\cite[Computing machinery and intelligence]{turing2009computing}). Il test prende in causa un interrogatore umano che fa la stessa domanda a due ascoltatori, una persona ed una macchina. Se l'interrogatopra non riesce a distingure l'uomo dalla macchina allora la macchina ha superato il test di Turing. Questo test è stato da sempre l'obiettivo dellàintelligenza artificiale anche se ad oggi alcuni dubbi stanno sorgendo. La conferenza del 1956 a Dartmouth (organizzata da Marvin Minsky, John McCarty, Claude Shannon e Nathan Rochester) è considerata il momento in cui l'AI è stata globalmente riconosciuta. 

Il percettorne di Resenblatt aveva fatto molto clamore ma le aspettative dell'opinione pubblica non furoino incontrate. Le aspettative del pubblico non furono assolutamente incontrata, tanto che nel 68 un paper di Minsky e Papert dimostro le forte limitazioni del percettrone, dove di fatto veniva descritto che il percettrone non sarebbe stato in grado di emulare una operazione di or esclusivo con due input (non erano linearmente separabili). Nel 73 una seconda pubblicazione di Lighthill sottolinea le forti aspettativde del pubblico e gli scarsi risultati ottenuti dal progresso. Dal 74 all'80 ci fu il primo invero dell'intelligenza artificale dove essenzialmente nulla accadde e tutti persereo l'interesse al riogurado (controllo bene le fonto che qua me la sono un po inventata ecco letsgoski). 

Nel 1985 l'inverdo dell'ai fini quando venne introdotto il Gradent Descent Optimization per minimizzare l'errore in una rete (Rumelhart, Hinton and Williams). Inoltre nell 86 Rumelhart e colleghi espansero il lavoro introducento il conetto di back-propagation nelle reti neurali multilivello (molti livelli di neuroni attaxccati tra loro). QUersto algoritmo rivoluziono le capacita di imparare delle reti. Il secondo inverno dell AI inizio negli anni 90, dove ci si rese conto che queste reti neurali non erano scalabili: cera troppa poca potenza di calcolo.

Questo inverno venne causato dalle aspettative sulle capacita delle reti neurali che pero non abdavanbo di paripasso con lo sviluppo della potenza computazionale. Per questo motivo i ricercatori si spostaro su altri alogirtimi, che richiedevano meno potenza di calcolo come le support evctor machines (1963, Vapnik and Chervonenkis). Quando quiesti algoritmi furono implementati in kernel non lineari nel 92 (kernl trick) si fu in grado di rsolvere iperpiano non lineari senza la necessita di requisiti computazionali elevati. 

Quando alla meta degli anni 90 la potenza computazionale crebbe, l'attenzione per la AI auimeto di conseguenza. Nel 97, IBM sviluppo un super computer, chiamato Deep Blue, che sconfisse Kasparov a scacchi. Quersto evento rese le reti neurali risolrgere, con l'introduzioned elle reti nmeurali convoluzionali.

Continua a pagina 397 del libro\,\cite{muthukrishnan2020brief}


\todo{Articoli:\,\cite{mcculloch1943logical}}
\todo{quando fai CNN dici che ML fa cagare per questi}


\begin{comment}
    Video Enkk:         https://www.youtube.com/watch?v=QB4FR8U0N6g&t=1885s
    Deep learning:      https://www.nature.com/articles/nature14539
    Storia:             https://books.google.it/books?hl=it&lr=&id=GpYFEAAAQBAJ&oi=fnd&pg=PA393&dq=History+of+artificial+intelligence&ots=V4Y3JAo1le&sig=vCrUSwvkSHjayS99EKLAHkEa_8s&redir_esc=y#v=onepage&q=History%20of%20artificial%20intelligence&f=false 
\end{comment}


